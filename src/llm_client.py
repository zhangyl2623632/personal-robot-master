# llm_client.py
# é€šç”¨å¤§æ¨¡å‹å®¢æˆ·ç«¯ï¼Œæ”¯æŒ DeepSeek / OpenAI / Qwen / Moonshot / SparkDesk ç­‰
# æ”¯æŒä¸¥æ ¼æ¨¡å¼ã€ç»“æ„åŒ–è¾“å‡ºã€å…ƒæ•°æ®å¢å¼ºã€æ„å›¾è¯†åˆ«

import os
import re
import logging
import requests
import time
import traceback
from requests.adapters import Retry, HTTPAdapter
import json
from typing import List, Dict, Any, Optional, Union, Generator
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

# å‡è®¾ä½ æœ‰ä¸€ä¸ª config æ¨¡å—ï¼Œè‹¥æ²¡æœ‰ï¼Œå¯æ›¿æ¢ä¸º dotenv æˆ–ç¡¬ç¼–ç 
try:
    from src.config import global_config
except ImportError:
    # å¦‚æœæ²¡æœ‰ configï¼Œæä¾›é»˜è®¤é…ç½®ï¼ˆä½ å¯è‡ªè¡Œä¿®æ”¹ï¼‰
    class MockConfig:
        MODEL_PROVIDER = "deepseek"  # å¯é€‰: deepseek, openai, qwen, moonshot, spark
        MODEL_NAME = "deepseek-chat"
        MODEL_URL = "https://api.deepseek.com/v1/chat/completions"
        API_KEY = os.getenv("DEEPSEEK_API_KEY") or "sk-xxx"
        TEMPERATURE = 0.1
        MAX_TOKENS = 2048
        TIMEOUT = 30

    # ä½¿ç”¨mocké…ç½®
    global_config = MockConfig()

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# å®šä¹‰æ¨¡å‹æä¾›å•†æšä¸¾
class ModelProvider(str, Enum):
    DEEPSEEK = "deepseek"
    OPENAI = "openai"
    QWEN = "qwen"
    MOONSHOT = "moonshot"
    SPARK = "spark"
    # å¯ä»¥ç»§ç»­æ‰©å±•

# å®šä¹‰ä¸€äº›å·¥å…·å‡½æ•°
# ======================== 
# ğŸ› ï¸ å·¥å…·å‡½æ•° 
# ======================== 
def preprocess_query(query: str) -> str:
    """é¢„å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
    # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œç¬¦
    processed = ' '.join(query.strip().split())
    # è½¬å°å†™ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦ï¼‰
    # processed = processed.lower()
    return processed

def enhance_context_with_metadata(context: str) -> str:
    """ç”¨å…ƒæ•°æ®å¢å¼ºä¸Šä¸‹æ–‡"""
    # æå–å…ƒæ•°æ®
    metadata = extract_metadata_from_text(context)
    
    # æ·»åŠ å…ƒæ•°æ®åˆ°ä¸Šä¸‹æ–‡
    enhanced_context = ""
    if 'title' in metadata:
        enhanced_context += f"ã€æ ‡é¢˜ã€‘{metadata['title']}\n\n"
    
    enhanced_context += context
    return enhanced_context

def extract_metadata_from_text(text: str) -> Dict[str, str]:
    """ä»æ–‡æœ¬ä¸­æå–å…ƒæ•°æ®"""
    metadata = {}
    lines = text.strip().split('\n')[:200]  # åªå¤„ç†å‰200è¡Œ

    # æ­£åˆ™æ¨¡å¼åŒ¹é…å¸¸è§å…ƒæ•°æ®
    patterns = {
        'version': r'(Version|ç‰ˆæœ¬)[\s:]+([\d\.]+)',
        'date': r'(Date|æ—¥æœŸ)[\s:]+([\d\-\/]+)',
        'author': r'(Author|ä½œè€…)[\s:]+([^\n]+)',
        'release': r'(Release|å‘å¸ƒ)[\s:]+([^\n]+)',
    }

    # æå–å…ƒæ•°æ®
    for key, pattern in patterns.items():
        for line in lines[:50]:  # åªåœ¨å‰50è¡ŒæŸ¥æ‰¾
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                metadata[key] = match.group(1).strip()

    # å¯å‘å¼æå–ä¸»æ ‡é¢˜
    if 'title' not in metadata:
        for line in lines[:10]:
            stripped = line.strip()
            if stripped and len(stripped) > 5 and not re.search(r'^(Version|Date|Author|Release|Prepared|Document)', stripped, re.I):
                metadata['title'] = stripped
                break

    # æå–å…³é”®æ®µè½ï¼ˆåŠŸèƒ½/é…ç½®/è´¹ç”¨/æµç¨‹ï¼‰
    key_sections = {
        'main_functions': [], 
        'system_config': [], 
        'fee_structure': [], 
        'process_flow': [],
        'data_structures': []
    }
    current_section = None

    for i, line in enumerate(lines[:200]):
        stripped = line.strip()
        if not stripped: continue

        # è¯†åˆ«ç« èŠ‚æ ‡é¢˜ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        if re.search(r'åŠŸèƒ½|Functions|Features', stripped, re.I):
            current_section = 'main_functions'
        elif re.search(r'é…ç½®|Configuration|Settings', stripped, re.I):
            current_section = 'system_config'
        elif re.search(r'è´¹ç”¨|Fee|Cost|Price|è´¹ç‡', stripped, re.I):
            current_section = 'fee_structure'
        elif re.search(r'æµç¨‹|Process|Steps|æ­¥éª¤', stripped, re.I):
            current_section = 'process_flow'
        elif re.search(r'æ•°æ®ç»“æ„|Data Structure|è¡¨æ ¼|Table', stripped, re.I):
            current_section = 'data_structures'
        elif re.search(r'^\d+\.', stripped):  # æ•°å­—æ ‡é¢˜
            pass  # ä¿æŒå½“å‰section
        elif stripped and current_section and not stripped.endswith(':') and i < len(lines) - 1 and lines[i+1].strip():
            # å°†å†…å®¹æ·»åŠ åˆ°å½“å‰section
            key_sections[current_section].append(stripped)

    # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    for key, value in key_sections.items():
        if value:
            metadata[key] = '\n'.join(value[:5])  # åªä¿ç•™å‰5ä¸ªé¡¹ç›®

    return metadata

# ======================== 
# ğŸ¯ å®¢æˆ·ç«¯æŠ½è±¡åŸºç±» 
# ======================== 
class BaseLLMClient(ABC):
    """LLM å®¢æˆ·ç«¯æŠ½è±¡åŸºç±»"""

    def __init__(self, config=None):
        self.config = config or global_config
        self.provider = ModelProvider(self.config.MODEL_PROVIDER.lower())
        # ä½¿ç”¨getattrç¡®ä¿å³ä½¿é…ç½®é¡¹ä¸å­˜åœ¨ä¹Ÿä¸ä¼šæŠ›å‡ºå¼‚å¸¸
        # æ ¹æ®æ¨¡å‹æä¾›å•†è·å–å¯¹åº”çš„APIå¯†é’¥
        if self.provider == ModelProvider.DEEPSEEK:
            self.api_key = getattr(self.config, 'DEEPSEEK_API_KEY', None) or os.getenv('DEEPSEEK_API_KEY')
        elif self.provider == ModelProvider.QWEN:
            self.api_key = getattr(self.config, 'QWEN_API_KEY', None) or os.getenv('QWEN_API_KEY')
        elif self.provider == ModelProvider.OPENAI:
            self.api_key = getattr(self.config, 'OPENAI_API_KEY', None) or os.getenv('OPENAI_API_KEY')
        elif self.provider == ModelProvider.MOONSHOT:
            self.api_key = getattr(self.config, 'MOONSHOT_API_KEY', None) or os.getenv('MOONSHOT_API_KEY')
        else:
            # é»˜è®¤ä½¿ç”¨é€šç”¨API_KEY
            self.api_key = getattr(self.config, 'API_KEY', None) or os.getenv('API_KEY')
        
        self.model_name = self.config.MODEL_NAME
        self.timeout = getattr(self.config, 'TIMEOUT', 30)
        self.temperature = getattr(self.config, 'TEMPERATURE', 0.1)
        self.max_tokens = getattr(self.config, 'MAX_TOKENS', 2048)
        
        # é‡è¯•é…ç½®
        self.max_retries = getattr(self.config, 'RETRY_MAX_ATTEMPTS', 3)
        self.backoff_factor = getattr(self.config, 'RETRY_BACKOFF_FACTOR', 1.5)
        self.status_forcelist = getattr(self.config, 'RETRY_STATUS_FORCELIST', [429, 500, 502, 503, 504])
        
        # åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ä¼šè¯
        self.session = self._create_session()
        
        # ä¿å­˜æœ€è¿‘ä¸€æ¬¡æˆåŠŸçš„å“åº”ä½œä¸ºå¤‡ç”¨
        self._last_successful_response = None
        
        # å¥åº·çŠ¶æ€æ ‡å¿—
        self._is_healthy = True

    def _create_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„requestsä¼šè¯"""
        session = requests.Session()
        retry = Retry(
            total=self.max_retries,
            backoff_factor=self.backoff_factor,
            status_forcelist=self.status_forcelist,
            allowed_methods=["POST"],  # åªå¯¹POSTè¯·æ±‚é‡è¯•
            raise_on_status=False  # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…å¤„ç†çŠ¶æ€ç 
        )
        adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        return session

    @abstractmethod
    def _get_headers(self) -> Dict[str, str]:
        pass

    @abstractmethod
    def _get_api_url(self) -> str:
        pass

    @abstractmethod
    def _build_payload(self, messages: List[Dict[str, str]], stream: bool = False) -> Dict[str, Any]:
        pass

    def generate_response(
        self,
        prompt: str,
        context: Optional[List[str]] = None,
        history: Optional[List[Dict[str, str]]] = None,
        stream: bool = False
    ) -> Union[Optional[str], Generator[str, None, None]]:  # æµå¼è¿”å›ç”Ÿæˆå™¨
        """ç”Ÿæˆå›ç­”ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        try:
            messages = self._build_messages(prompt, context, history)
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            if stream:
                return self._call_api_with_retry_stream(messages)
            else:
                response = self._call_api_with_retry(messages)
                
                # è®°å½•å“åº”æ—¶é—´
                response_time = time.time() - start_time
                logger.info(f"ç”Ÿæˆå›ç­”æˆåŠŸï¼Œé•¿åº¦: {len(response) if response else 0} å­—ç¬¦ï¼Œå“åº”æ—¶é—´: {response_time:.2f}ç§’")
                
                # ä¿å­˜æˆåŠŸçš„å“åº”
                if response:
                    self._last_successful_response = response
                    self._is_healthy = True
                
                return response
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”å¼‚å¸¸: {str(e)}")
            traceback.print_exc()
            # è¿”å›ä¸Šæ¬¡æˆåŠŸçš„å“åº”ä½œä¸ºåå¤‡
            if self._last_successful_response:
                logger.warning("ä½¿ç”¨ä¸Šæ¬¡æˆåŠŸçš„å“åº”ä½œä¸ºåå¤‡")
                return self._last_successful_response
            # æä¾›é»˜è®¤å›å¤
            return self._get_default_response(prompt)

    def _get_default_response(self, prompt: str) -> str:
        """è·å–é»˜è®¤å›å¤ï¼Œå½“æ‰€æœ‰APIè°ƒç”¨éƒ½å¤±è´¥æ—¶"""
        self._is_healthy = False
        logger.warning(f"APIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤å›å¤")
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹è¿”å›ä¸åŒçš„é»˜è®¤å›å¤
        query_lower = prompt.lower()
        if any(word in query_lower for word in ["æ¦‚è¿°", "ä»‹ç»", "æ€»ç»“", "ä¸»è¦å†…å®¹"]):
            return "æ ¹æ®ç°æœ‰èµ„æ–™ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚å½“å‰æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        elif any(word in query_lower for word in ["è¡¨æ ¼", "è´¹ç‡", "ä»·æ ¼", "è´¹ç”¨"]):
            return "æ ¹æ®ç°æœ‰èµ„æ–™ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚å½“å‰æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        elif any(word in query_lower for word in ["æµç¨‹", "æ­¥éª¤", "å¦‚ä½•"]):
            return "æ ¹æ®ç°æœ‰èµ„æ–™ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚å½“å‰æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
        else:
            return "æ ¹æ®ç°æœ‰èµ„æ–™ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚å½“å‰æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"

    def _build_messages(
        self,
        prompt: str,
        context: Optional[List[str]] = None,
        history: Optional[List[Dict[str, str]]] = None
    ) -> List[Dict[str, str]]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = [{"role": "system", "content": global_config.SYSTEM_PROMPT}]

        if history:
            messages.extend(history)

        processed_query = preprocess_query(prompt)

        if context:
            enhanced_contexts = []
            for i, ctx in enumerate(context):
                enhanced_ctx = enhance_context_with_metadata(ctx)
                enhanced_contexts.append(f"ã€ä¸Šä¸‹æ–‡ {i+1}ã€‘\n{enhanced_ctx}")

            context_text = "\n\n".join(enhanced_contexts)
            user_content = f"{context_text}\n\n---\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{processed_query}"
        else:
            user_content = processed_query

        messages.append({"role": "user", "content": user_content})
        return messages

    def _call_api_with_retry(self, messages: List[Dict[str, str]]) -> Optional[str]:
        """å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨ï¼ˆéæµå¼ï¼‰"""
        attempt = 0
        while attempt < self.max_retries:
            attempt += 1
            try:
                response = self._call_api(messages)
                if response and "choices" in response and len(response["choices"]) > 0:
                    content = response["choices"][0]["message"]["content"]
                    logger.info(f"APIè°ƒç”¨æˆåŠŸ (å°è¯• {attempt}/{self.max_retries})")
                    return content
                else:
                    if attempt < self.max_retries:
                        wait_time = self.backoff_factor ** (attempt - 1) + 0.5 * (attempt - 1)
                        logger.warning(f"APIå“åº”æ ¼å¼å¼‚å¸¸ï¼Œ{wait_time:.2f}ç§’åé‡è¯• (å°è¯• {attempt}/{self.max_retries})")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({self.max_retries})")
                        return None
            except Exception as e:
                if attempt < self.max_retries:
                    wait_time = self.backoff_factor ** (attempt - 1) + 0.5 * (attempt - 1)
                    logger.warning(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}, {wait_time:.2f}ç§’åé‡è¯• (å°è¯• {attempt}/{self.max_retries})")
                    time.sleep(wait_time)
                else:
                    logger.error(f"APIè°ƒç”¨å¼‚å¸¸ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({self.max_retries}): {str(e)}")
                    traceback.print_exc()
                    return None
        return None

    def _call_api_with_retry_stream(self, messages: List[Dict[str, str]]) -> Generator[str, None, None]:
        """å¸¦é‡è¯•æœºåˆ¶çš„æµå¼APIè°ƒç”¨"""
        attempt = 0
        while attempt < self.max_retries:
            attempt += 1
            try:
                logger.info(f"å¼€å§‹æµå¼APIè°ƒç”¨ (å°è¯• {attempt}/{self.max_retries})")
                # è·å–æµå¼ç”Ÿæˆå™¨
                stream_generator = self._call_api_stream(messages)
                
                # å¦‚æœç”Ÿæˆå™¨å­˜åœ¨ï¼Œåˆ™é€å—yieldå†…å®¹
                if stream_generator:
                    for chunk in stream_generator:
                        yield chunk
                    # å¦‚æœæˆåŠŸyieldå®Œæ‰€æœ‰å†…å®¹ï¼Œè¿”å›
                    logger.info(f"æµå¼APIè°ƒç”¨æˆåŠŸå®Œæˆ (å°è¯• {attempt}/{self.max_retries})")
                    return
                
            except Exception as e:
                logger.error(f"æµå¼APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
                traceback.print_exc()
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…å¹¶é‡è¯•
            if attempt < self.max_retries:
                wait_time = self.backoff_factor ** (attempt - 1) + 0.5 * (attempt - 1)
                logger.warning(f"æµå¼APIè°ƒç”¨å¤±è´¥ï¼Œ{wait_time:.2f}ç§’åé‡è¯• (å°è¯• {attempt}/{self.max_retries})")
                time.sleep(wait_time)
            else:
                logger.error(f"æµå¼APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({self.max_retries})")
                # æä¾›é»˜è®¤å›å¤
                yield "å¾ˆæŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ä¸ºæ‚¨æä¾›è¯¥ä¿¡æ¯ã€‚è¯·ç¨åå†è¯•ã€‚"

    def validate_api_key(self) -> bool:
        """éªŒè¯ API å¯†é’¥æ˜¯å¦æœ‰æ•ˆ"""
        if not self.api_key:
            return False
        test_messages = [{"role": "user", "content": "Hello"}]
        try:
            resp = self._call_api(test_messages)
            return resp is not None
        except Exception:
            return False

    def is_healthy(self) -> bool:
        """æ£€æŸ¥APIæœåŠ¡æ˜¯å¦å¥åº·"""
        return self._is_healthy

    def refresh_client(self):
        """åˆ·æ–°å®¢æˆ·ç«¯å®ä¾‹ï¼Œé‡æ–°ä»å·¥å‚åˆ›å»º"""
        # é‡æ–°å¯¼å…¥æœ€æ–°çš„å…¨å±€é…ç½®ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹è®¾ç½®
        from src.config import global_config
        # åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹
        new_client = LLMClientFactory.create_client(global_config)
        
        # ç›´æ¥æ›´æ–°å…¨å±€å®¢æˆ·ç«¯å®ä¾‹ï¼Œé¿å…æ¨¡å—å¯¼å…¥é—®é¢˜
        import src.llm_client
        src.llm_client.llm_client = new_client
        
        # åŒæ—¶æ›´æ–°å¯èƒ½å¼•ç”¨äº†è¯¥å®¢æˆ·ç«¯çš„å…¶ä»–æ¨¡å—
        try:
            import src.web_interface
            if hasattr(src.web_interface, 'llm_client'):
                src.web_interface.llm_client = new_client
        except ImportError:
            pass  # å¦‚æœweb_interfaceæ¨¡å—ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯
        
        # ç‰¹åˆ«æ›´æ–°RAGPipelineä¸­çš„å®¢æˆ·ç«¯å®ä¾‹ï¼Œå› ä¸ºå®ƒåœ¨åˆå§‹åŒ–æ—¶ä¿å­˜äº†å¯¹æ—§å®¢æˆ·ç«¯çš„å¼•ç”¨
        try:
            from src.rag_pipeline import rag_pipeline
            if hasattr(rag_pipeline, 'llm_client'):
                rag_pipeline.llm_client = new_client
                logger.info("å·²æˆåŠŸæ›´æ–°RAGPipelineä¸­çš„å®¢æˆ·ç«¯å®ä¾‹")
        except ImportError:
            pass  # å¦‚æœrag_pipelineæ¨¡å—ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯
        
        logger.info(f"å®¢æˆ·ç«¯å·²åˆ·æ–°: æ–°æä¾›å•†={new_client.provider}, æ–°æ¨¡å‹={new_client.model_name}")

    def _call_api(self, messages: List[Dict[str, str]]) -> Optional[Dict[str, Any]]:
        """æ‰§è¡Œå®é™…çš„APIè°ƒç”¨"""
        try:
            # åœ¨è°ƒç”¨æ¨¡å‹å‰æ‰“å°æ—¥å¿—ï¼Œæ˜¾ç¤ºä½¿ç”¨çš„æ¨¡å‹å’Œæ¨¡å‹keyçš„é…ç½®æƒ…å†µ
            logger.info(f"å‡†å¤‡è°ƒç”¨æ¨¡å‹ - æä¾›å•†: {self.provider}, æ¨¡å‹åç§°: {self.model_name}")
            logger.info(f"APIå¯†é’¥çŠ¶æ€: {'å·²é…ç½®' if self.api_key else 'æœªé…ç½®'}")
            
            url = self._get_api_url()
            headers = self._get_headers()
            payload = self._build_payload(messages, stream=False)
            
            logger.debug(f"APIè°ƒç”¨: URL={url}, æ¶ˆæ¯æ•°é‡={len(messages)}")
            
            # å‘é€è¯·æ±‚
            response = self.session.post(
                url, 
                headers=headers, 
                json=payload, 
                timeout=self.timeout
            )
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if response.status_code == 200:
                return response.json()
            else:
                logger.error(f"APIè°ƒç”¨å¤±è´¥: çŠ¶æ€ç  {response.status_code}")
                logger.error(f"å“åº”å†…å®¹: {response.text}")
                return None
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            traceback.print_exc()
            return None

    def _call_api_stream(self, messages: List[Dict[str, str]]) -> Generator[str, None, None]:
        """æ‰§è¡Œæµå¼APIè°ƒç”¨"""
        try:
            url = self._get_api_url()
            headers = self._get_headers()
            payload = self._build_payload(messages, stream=True)
            
            logger.debug(f"æµå¼APIè°ƒç”¨: URL={url}, æ¶ˆæ¯æ•°é‡={len(messages)}")
            
            # å‘é€æµå¼è¯·æ±‚
            with self.session.post(
                url, 
                headers=headers, 
                json=payload, 
                stream=True, 
                timeout=self.timeout
            ) as response:
                if response.status_code == 200:
                    # å¤„ç†æµå¼å“åº”
                    for chunk in response.iter_lines():
                        if chunk:
                            # ç§»é™¤ 'data: ' å‰ç¼€å¹¶è§£æJSON
                            try:
                                chunk_str = chunk.decode('utf-8')
                                if chunk_str.startswith('data: '):
                                    chunk_str = chunk_str[6:]
                                if chunk_str.strip() == '[DONE]':
                                    break
                                data = json.loads(chunk_str)
                                if 'choices' in data and data['choices']:
                                    delta = data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        yield delta['content']
                            except json.JSONDecodeError:
                                logger.warning(f"æ— æ³•è§£ææµå¼å“åº”å—: {chunk_str}")
                else:
                    logger.error(f"æµå¼APIè°ƒç”¨å¤±è´¥: çŠ¶æ€ç  {response.status_code}")
                    logger.error(f"å“åº”å†…å®¹: {response.text}")
        except Exception as e:
            logger.error(f"æµå¼APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            traceback.print_exc()

# ========================
# ğŸš€ DeepSeek å®¢æˆ·ç«¯å®ç°
# ========================
class DeepSeekClient(BaseLLMClient):
    def _get_headers(self) -> Dict[str, str]:
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

    def _get_api_url(self) -> str:
        return getattr(self.config, 'MODEL_URL', "https://api.deepseek.com/v1/chat/completions")

    def _build_payload(self, messages: List[Dict[str, str]], stream: bool = False) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stream": stream
        }


# ========================
# ğŸ§© OpenAI / å…¼å®¹å®¢æˆ·ç«¯
# ========================
class OpenAIClient(BaseLLMClient):
    def _get_headers(self) -> Dict[str, str]:
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

    def _get_api_url(self) -> str:
        return getattr(self.config, 'MODEL_URL', "https://api.openai.com/v1/chat/completions")

    def _build_payload(self, messages: List[Dict[str, str]], stream: bool = False) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stream": stream
        }


# ========================
# ğŸŒ™ Moonshot (æœˆä¹‹æš—é¢) å®¢æˆ·ç«¯
# ========================
class MoonshotClient(BaseLLMClient):
    def _get_headers(self) -> Dict[str, str]:
        return {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

    def _get_api_url(self) -> str:
        return getattr(self.config, 'MODEL_URL', "https://api.moonshot.cn/v1/chat/completions")

    def _build_payload(self, messages: List[Dict[str, str]], stream: bool = False) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stream": stream
        }

# ========================
# ğŸ¯ Qwen (é€šä¹‰åƒé—®) HTTPå®¢æˆ·ç«¯ â€”â€” æœ€ç»ˆä¿®å¤ç‰ˆ
# ========================
class QwenClient(BaseLLMClient):

    def __init__(self, config=None):
        # ç¡®ä¿ä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œä¸ä½¿ç”¨é»˜è®¤å‚æ•°ä»¥é¿å…å¼•ç”¨æ—§é…ç½®
        if config is None:
            from src.config import global_config
            config = global_config
            
        super().__init__(config)
        
        # çˆ¶ç±»å·²ç»å¤„ç†äº†APIå¯†é’¥çš„è·å–ï¼Œä½†æˆ‘ä»¬å†ç¡®è®¤ä¸€æ¬¡
        if not self.api_key:
            logger.warning("æœªè®¾ç½® QWEN_API_KEYï¼Œè°ƒç”¨å¯èƒ½å¤±è´¥ï¼")

    
    def _get_headers(self) -> Dict[str, str]:

        return {

            "Content-Type": "application/json",

            "Authorization": f"Bearer {self.api_key}"

        }

    
    def _get_api_url(self) -> str:

        # ä½¿ç”¨ DashScope å…¼å®¹ OpenAI çš„æ¥å£

        return getattr(self.config, 'MODEL_URL', "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions")

    
    def _build_payload(self, messages: List[Dict[str, str]], stream: bool = False) -> Dict[str, Any]:

        # ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼

        return {

            "model": self.model_name,  # å¦‚ "qwen-plus", "qwen-turbo", "qwen-max"

            "messages": messages,

            "temperature": self.temperature,

            "max_tokens": self.max_tokens,

            "stream": stream

        }

# ========================
# ğŸ¯ å·¥å‚ç±»ï¼šæ ¹æ®é…ç½®åˆ›å»ºå®¢æˆ·ç«¯
# ========================
class LLMClientFactory:
    @staticmethod
    def create_client(config=None) -> BaseLLMClient:
        config = config or global_config
        provider = config.MODEL_PROVIDER.lower()

        client_map = {
            "deepseek": DeepSeekClient,
            "openai": OpenAIClient,
            "moonshot": MoonshotClient,
            "qwen": QwenClient,
            # "qwen_dashscope": QwenDashScopeClient,  # æš‚æœªå®ç°
            # å¯ç»§ç»­æ‰©å±•
        }

        if provider in client_map:
            return client_map[provider](config)
        else:
            logger.warning(f"æœªçŸ¥æ¨¡å‹æä¾›å•†: {provider}ï¼Œé»˜è®¤ä½¿ç”¨ DeepSeekClient")
            return DeepSeekClient(config)

# åˆ›å»ºLLMå®¢æˆ·ç«¯å®ä¾‹
llm_client = LLMClientFactory.create_client()